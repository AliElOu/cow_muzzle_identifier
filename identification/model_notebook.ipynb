{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce933c2e",
   "metadata": {},
   "source": [
    "# Import Required Libraries\n",
    "This cell imports necessary libraries for model creation, data preprocessing, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "030b78a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, Flatten, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import json\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "import os\n",
    "from glob import glob\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ba7bf",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86709fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "x = Flatten()(base_model.output)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(268, activation='softmax')(x)\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "model.compile(optimizer=Adam(1e-4), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c2b60d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3328 images belonging to 268 classes.\n",
      "Found 643 images belonging to 268 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "val_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    \"./train\",\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=16\n",
    ")\n",
    "\n",
    "train_classes = train_generator.class_indices.keys()\n",
    "\n",
    "val_generator = val_datagen.flow_from_directory(\n",
    "    \"./valid\",\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=16,\n",
    "    classes=list(train_classes)\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92719a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ali_e\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\trainers\\data_adapters\\py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m232s\u001b[0m 1s/step - accuracy: 0.0453 - loss: 5.4188 - val_accuracy: 0.2862 - val_loss: 4.3747\n",
      "Epoch 2/50\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 1s/step - accuracy: 0.2839 - loss: 4.1595 - val_accuracy: 0.5801 - val_loss: 3.1879\n",
      "Epoch 3/50\n",
      "\u001b[1m208/208\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 888ms/step - accuracy: 0.5093 - loss: 3.0625"
     ]
    }
   ],
   "source": [
    "model.fit(\n",
    "      train_generator,\n",
    "      validation_data=val_generator,\n",
    "      epochs=50\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abc22bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('muzzle.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca9b1d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(\"muzzle.keras\")\n",
    "embedding_model = Model(inputs=model.input, outputs=model.layers[-2].output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf3174e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(img_path):\n",
    "    img = image.load_img(img_path, target_size=(224, 224))\n",
    "    img_array = image.img_to_array(img) / 255.0\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    embedding = embedding_model.predict(img_array)\n",
    "    return embedding.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7420f794",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_embedding_database(train_dir=\"./train\"):\n",
    "    database = {\"embeddings\": [], \"labels\": []}\n",
    "    for class_name in os.listdir(train_dir):\n",
    "        class_path = os.path.join(train_dir, class_name)\n",
    "        if not os.path.isdir(class_path): continue\n",
    "        for img_path in glob(os.path.join(class_path, \"*.jpg\")):\n",
    "            emb = get_embedding(img_path)\n",
    "            database[\"embeddings\"].append(emb)\n",
    "            database[\"labels\"].append(class_name)\n",
    "    return database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db40899",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_identity(img_path, database, seuil=0.7):\n",
    "    query_emb = get_embedding(img_path)\n",
    "    sims = cosine_similarity([query_emb], database[\"embeddings\"])[0]\n",
    "    best_score = np.max(sims)\n",
    "    best_index = np.argmax(sims)\n",
    "    \n",
    "    if best_score < seuil:\n",
    "        return \"INCONNUE\", best_score\n",
    "    else:\n",
    "        return database[\"labels\"][best_index], best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f012f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# database = build_embedding_database(\"./train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521b9dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_new_cow_to_database(cow_id, image_paths, database):\n",
    "    for img_path in os.listdir(image_paths):\n",
    "        emb = get_embedding(os.path.join(image_paths, img_path))\n",
    "        database[\"embeddings\"].append(emb)\n",
    "        database[\"labels\"].append(cow_id)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
